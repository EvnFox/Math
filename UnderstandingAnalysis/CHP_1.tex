\documentclass[11pt,largemargins]{homework}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\ran}{\operatorname{ran}}
\newcommand{\dom}{\operatorname{dom}}
\newcommand{\eps}{\varepsilon}
\newcommand{\ssd}{\bigtriangleup}
\newcommand{\pow}{\mathcal{P}}

% TODO: replace these with your information
\newcommand{\hwname}{}
\newcommand{\hwemail}{}
\newcommand{\hwtype}{Chapter}
\newcommand{\hwnum}{1}
\newcommand{\hwclass}{Understanding Analysis}
\newcommand{\hwlecture}{}
\newcommand{\hwsection}{}

\begin{document}
\maketitle
The first part of chapter one just went over some prelims/reviews. Here are my solutions to selected exercises. 
\question
    \begin{alphaparts}
        \questionpart
            Prove that the $\sqrt{3} \notin \Q$
            
            First, I prove that $3|x^2 \implies 3|x$ as I will need this fact in my proof of the irrationality of $\sqrt{3}$. 
            
            \begin{proof}
                We use the contrapositive, so assume that $3\nmid x$. Then there exists $k \in \Z$ such that $x = 3k + 1$ or $ x = 3k + 2$. Then note that if $x = 3k + 1$, then 
                \[ x = 3k + 1 \] 
                \[ x^2 = 9k^2 + 6k + 1 = 3(3k^2 + 2k) + 1\]
                
                and in the other case we have
                
                \[ x = 3k + 2 \] 
                \[ x^2 = 9k^2 + 12k + 4 = 3(3k^2 + 4k + 1) + 1 \] 
                
                So, in both cases we see that $3\nmid x^2$ as desired. 
            \end{proof}
            
            Now I prove that $\sqrt{3} \notin \Q$ 
            \begin{proof}
                For the sake of contradiction, assume that $\sqrt{3} \in \Q$. Then we may fix $m, n \in \Q$ such that $\sqrt{3} = \frac{m}{n}$. We then have that, 
                
                \[ 3 = \left(\frac{m}{n}\right)^2 \] 
                
                by the fundamental theorem of arithmetic, we may write $m^2$ and $n^2$ in terms of their prime factors and cancel any factor(s) that they have in common, i.e., we reduce $\frac{m}{n}$ such that they have no common factors. Since $m$ and $n$ have no common factors we note that they cannot both be divisible by 3. Fist observe that 
                \begin{equation}
                    3n^2 = m^2
                \end{equation}
                
                
                and hence we have $3| m^2$ which implies $3|m$; so we fix $k \in \Z$ such that $ m = 3k$ and then we substitute this expression for $m$ back into (1). 
                
                \begin{equation}
                     3n^2 = (3k)^2 = 9k^2
                \end{equation}
                
                \begin{equation}
                     n^2 = 3k^2 
                \end{equation}
                
                and we have that 3 divides $n^2$ and by extension 3 divides $n$. Thus we have a contradiction as desired.  
            \end{proof}
        \questionpart
            Does a similar argument work to prove that $\sqrt{6} \notin \Q$? Where does the proof breakdown for $\sqrt{4}$?
            
                Yes, whether or not this method works for $\sqrt{x}$ is related to the prime factorization of $x$, since the prime factors of 6 both have an exponent of 1, $6|m^2 \implies 6|m$ will hold. This is because if 2 and 3 are prime factors of $m^2$ then they will have to be prime factors of m, otherwise how would they have been prime factors of $m^2$? The point is squaring a number doesn't add new prime factors; it just multiples the exponent of each prime factor by 2. When we try to apply this argument to $\sqrt{4}$ the problem is that $4|m^2 \implies 4|m$ doesn't hold since the exponent of the prime factor of 4 is 2. To give an example note that $4|36 = 6^2 = 2^23^2$ but $4\nmid 6 = 3 (2) $. 
    \end{alphaparts}
    
    \question
    Prove that there is no rational number satisfying $2^r = 3$. 
    
    \begin{proof}
    We use contradiction, so assume that there exists $r \in \Q$ such that $2^r = 3$. Then since $r$ is rational by assumption we may fix $m, n \in \Q$ such that $r = \frac{m}{n}$. Substituting in for $r$ gives
    
    \[ 2^{\frac{m}{n}}  = 3\] 
    
    then we raise both sides to the $n^{\text{th}}$ power and get 
    
    \[ 2^m = 3^n \] 
    
    which contradicts the uniqueness of the fundamental theorem of arithmetic. 
    \end{proof}


\question
The triangle inequality is given by $|a+b| \leq |a| + |b| $.
    \begin{alphaparts}
        \questionpart
            Verify the triangle inequality in the special case that $a$ and $b$ have the same sign. 
            
            \begin{proof}
                Let $a,b \in \R$ and assume that $a $ and $b$ have the same sign. Then if $a$ and $b$ are both negitive we see that $|a + b| = a + b$ and if $a$ and $b$ are both positive then $|a + b| = a + b$. Then note that regardless of the signs of $a$ and $b$, $|a| + |b| = a + b$.
                It is then clear that the inequality holds. 
            \end{proof}
        \questionpart
            Give a general proof the the triangle inequality. 

            \begin{proof}
                First we prove that $(a + b)^2 \leq (|a| + |b|)^2 $ by observing $ab \leq |ab| $. Then we multiply both sides by $2$ and obtain $2ab \leq 2|ab|$, we then add $a^2 + b^2$ to both sides and get $a^2 + 2ab + b^2 \leq |a|^2 + 2|ab| + |b|^2$. Factoring this gives 
                $(a + b)^2  \leq (|a| + |b|)^2$. We can now use this to prove the triangle inequality by taking the square root of both sides which yeilds, 

                \[
                    |a + b| \leq ||a| + |b|| 
                \]

                but since $|a| + |b| \geq 0$ we have 

                \[
                    |a + b| \leq |a| + |b|
                \]

                as desired. 
            
            \end{proof}

        \questionpart
            Use the triangle inequality to prove that $|a - b| \leq |a - c| + |c - d| + |d - b|$. 

            \begin{proof}
                Let $x = (a - c)$ and let $y = (c - d) + (d - b)$, the the triangle inequality tells us that 

                \[ |x + y| \leq |x| + |y| \] 
                \[ |a - b| \leq |a - c| + |(c - d) + (d - b)| \] 

                Now we can apply the triangle inequality again to the second term on the right hand side of the last 
                equation. So we let $z = (c - d)$ and let $w = (d - b)$, by the triangle inequality we the have $|c - b| \leq |c - d| + |d - b|$. 
                Substituting back in gives, 

                \[ |a - b| \leq |a - c| + |(c - d) + (d - b)| = |a - c| + |c - d| + |d - b| \] 
                \[|a - b| \leq |a - c| + |c - d| + |d - b|\]

                as desired. 

            \end{proof}

            
    \end{alphaparts}


\question
Given a function $f$ and a subset of its domain $A$, let $f[A]$ denote the range of $f$ over $A$, i.e., $f[A] = \{f(x) | x \in A \} $

\begin{alphaparts}
    \questionpart
    let $f: \R \to \R$ given by $f(x) = x^2$. Let $A = [0, 2]$ and let $B = [1, 4]$. Does $f[A \cap B] = f[A] \cap f[B]$? What about $f[A \cup B] = f[A] \cup f[B]$? 

    
    \begin{proof}
        Both parts of this question are true. First I prove $f[A \cap B] = f[A] \cap f[B]$. First note that $A \cap B = [1, 2]$ then we may compute the range of $f$ on this domain and we get that $\ran f = [1, 4]$. 

       To prove that $\ran f = [1,4]$ first we prove that $\ran f \subseteq [1, 4]$ so let $y \in \ran f$. Then we fix $x \in \dom f$ such that $y = f(x) = x^2$. Then since $f$ is increasing on the interval $[1,2]$ it is clear that $y \in [1, 4]$. 
       Now to prove that $[1,4] \subseteq \ran f$ let $y \in [1, 4]$ be arbitrary. Then we must find a $x \in \dom f$ such that $f(x) = y$. Choosing $x = \sqrt{y}$ gives the desired result. Since $f(\sqrt{y}) = (\sqrt{y})^2 = y$. Since $\sqrt{y}$ is well defined on $[0, 4]$ 
       we conclude that $\ran f = [1, 4]$. Hence $f[A \cap B] = [1, 4]$. 
    \end{proof}

    \questionpart 
    Give an example where $f[A \cap B] = f[A] \cap f[B]$ doesn't hold. 

    Example: let $A = [-1, -2]$ and $B = [1, 2]$. 

    \questionpart 
    Let $A,B \subseteq \R$ and prove that for an arbitrary function $g: \R \to \R$, $g[A \cap B] \subseteq g[A] \cap g[B]$.

    \begin{proof}
        Let $y \in g[A \cap B]$ be arbitrary. Then there exists an $x \in A \cap B$ such that $y = g(x)$. Since $x \in A \cap B$ we know that $x \in A$ and $x \in B$. We also have that $y = g(x)$. 
        Hence $y \in g[A] $ and $y \in g[B]$. Then we have that $y \in g[A] \cap g[B]$.  
    \end{proof}

    *side note: if I add the condidtion that $g$ be injective, I think that you could prove $g[A \cap B] = g[A] \cap g[B]$. 
    A counter example existed for $f(x) = x^2$ only because it is not injective (I think). 

    \questionpart 
    Form a conjecture about $g[A \cup B]$ and $g[A] \cup g[B]$ and prove it. 

    Conjecture: $g[A \cup B] = g[A] \cup g[B]$. 

    \begin{proof}
        Let $y \in g[A \cup B]$ then we may fix $x \in A \cup B$ such that $y = g(x)$. Since we have that $x \in A$ or $x \in B$, we know that either $y \in g[A] $ or $y \in [B]$. It is then clear that $y$ is in the union. 
        To prove the other direction assume that $y \in g[A] \cup g[B]$, then either $y \in g[A] $ or $y \in g[B]$. If $y \in g[A]$ then we may fix $x \in A$ such that $y = g(x)$. Then it follows that $x \in A \cup B$; So then $y \in g[A \cup B]$. If it is not the case 
        that $y \in g[A]$ then $y \in g[B]$ must be true and the argument is the same. 
    \end{proof}



\end{alphaparts}

\question
Let $A, B \subseteq \R$ and let $g: D \to \R$ be arbitrary. Then we define $g^{-1}[B] = \{x \in D | g(x) \in B \} $. Prove that $g^{-1}[A \cap B] = g^{-1}[A] \cap g^{-1}[B]$. 

\begin{proof}
    Let $x \in g^{-1}[A \cap B]$, then we may fix $g(x) \in A \cap B$ such that $x \mapsto g(x)$. Then since $g(x) \in A \cap B$ we have that $g(x) \in A $ and $g(x) \in B$. It then follows by definition that 
    $x \in g^{-1}[A] $ and $x \in g^{-1}[B]$; then again by definition we have $x \in g^{-1}[A]  \cap g^{-1}[B]$. 
    
    To prove the other direction, let $x \in  g^{-1}[A]  \cap g^{-1}[B]$. Since $x \mapsto g(x) $ we have that $g(x) \in A$ and $g(x) \in B$. Thus $g(x) \in A \cap B$. Then by defintion we have that $x \in g^{-1}[A \cap B]$ as desired. 
    
\end{proof}

\question
let $y_1 = 6$ then for all $n \in \N$ define $y_{n + 1} = \frac{2y_n - 6}{3}$. 

\begin{alphaparts}
    \questionpart
    Prove that $y_n > 6$ for all $n \in \N$

    \begin{proof}
        We proceed with the principal of mathematical induction. 
        \begin{induction}
            \basecase
            We show that $y_2 > -6$, since we already have $y_1 = 6$ we compute $y_2$ using the definition and we get 
            \[ y_2 = \frac{2(y_1) - 6}{3} = \frac{2(6)- 6}{3} = 2 > -6\] 
            as desired. 

            \indhyp
            Assume that for some $k \geq 2$ that we have $y_k > -6$ we want to show that $y_{k + 1} > -6$. 

            \indstep
            we have that $y_k > -6$ and can use simple algebra to get to the $y_{k+1}$ case. 

            \[y_k > -6 \]
            \[ 2y_k > -12 \] 
            \[ 2y_k - 6 > -18 \] 
            \[ \frac{2y_k - 6}{3} > -6 \] 
            \[y_{k+1} > -6 \] 

            and so by the principal of mathematical induction the proposistion holds. 
        \end{induction}
    \end{proof}

    \questionpart
    Show that the sequence $\{y_1, y_2, y_3, ... \}$ is decreasing. 
    
    \begin{proof}
        To prove that the sequence is decreasing we show that for any $n \in \N$ we have $y_n > y_{n+1}$. By the previous result 
        we have that $y_n > -6$ for all $n \in \N$. Then it follows, 

        \[3y_n - 2y_n > -6 \] 
        \[3y_n > 2y_n - 6 \] 
        \[ y_n > \frac{2y_n -6}{3} \] 

        but then by defintion of the $n + 1$th term we get 

        \[ y_n > y_{n+1} \] 
        
        as desired. 
    
    \end{proof}
\end{alphaparts}

\question 
We have DeMorgan's laws given by 

\[A^c \cap B^c = (A \cup B)^c \] 
and 
\[A^c \cup B^c = (A \cap B)^c \] 

 

\begin{alphaparts}
    \questionpart
    use induction to prove DeMorgan's laws for an arbitrary number of unions/intersections with $n > 1$. 

    \[\bigcap_{i =1}^n A_i^c = \left(\bigcup_{i = 1}^n A_i \right)^c \]

    \begin{proof}
        \begin{induction}
            \basecase 
            For a basecase of $n = 2$ we simply have DeMorgan's laws, to prove that $A_1^c \cap A_2^c = (A_1 \cup A_2)^c$
            first let $x \in A_1^c \cap A_2^c$ then we have that $x \in A_1^c $ and that $x \in A_2^c$. By definiton this gives 
            $x \notin A_1$ and $x \notin A_2$. It then follows that $x \notin A_1 \cup A_2$. Which then by definiton implies
            $x \in (A_1 \cup A_2)^c $. The proof of the other direction is similar. 
            
            \indhyp
            Assume that for some $k \geq 2$ that we have $\bigcap_{i =1}^k A_i^c = \left(\bigcup_{i = 1}^k A_i \right)^c $.
            
            \indstep
            Since we have  $\bigcap_{i =1}^k A_i^c = \left(\bigcup_{i = 1}^k A_i \right)^c $ we intersect both sides with $A_{k + 1}^c$ and get 

            \[ \left( \bigcap_{i =1}^k A_i^c  \right) \cap A_{k+1}^c = \left(\bigcup_{i = 1}^k A_i \right)^c \cap A_{k+1}^c \] 
            \[\bigcap_{i =1}^{k+1} A_i^c = \left(\bigcup_{i = 1}^k A_i \right)^c \cap A_{k+1}^c \]

            to deal with the right hand side we simply apply DeMorgan's law as proven in the basecase. So we have that $\left(\bigcup_{i = 1}^k A_i \right)^c \cap A_{k+1}^c  = \left( \bigcup_{i = 1}^k A_i \cup A_{k+1} \right)^c$. Now Substituting in yeilds 

            \[\bigcap_{i =1}^{k+1} A_i^c = \left(\bigcup_{i = 1}^{k+1} A_i \right)^c \]

            as desired. 
            
        \end{induction}
    \end{proof}

    \questionpart
    Give an example showing that induction cannot be used to imply the validity of the infinite case.

    \begin{proof}
        Consider $B_i = (0, \frac{1}{i})$ we use induction to show that for all $n \in \N$ with $n > 1$ the intersection of $B_i$ is non-empty. 

        \begin{induction}
            \basecase
            For $n = 2$ we have that $\bigcap_{i = 1}^2 B_i = (0,\frac{1}{2})$. 

            \indhyp
            Assume that for some $k \geq 2$ that we have $\bigcap_{i = 1}^k B_i \neq \varnothing$.

            \indstep
            We know that $\bigcap_{i = 1}^k B_i \neq \varnothing$, now observe that for all $n \in \N$ the fraction $\frac{1}{n} > 0$. Also note that 
            $\frac{1}{n+1} < \frac{1}{n}$. Hence it follows that $B_{k+1} \neq \varnothing$ and that $B_{k+1} \subseteq B_k \subseteq \bigcap_{i = 1}^k B_i$. 
            then we have that 

            \[\bigcap_{i = 1}^k B_i \cap B_{k+1} \neq \varnothing \]
            \[\bigcap_{i =1}^{k+1} B_i \neq \varnothing \]

            as desired. 
                
            
        \end{induction}

        Now we show that $\bigcap_{i = 1}^{\infty} B_i = \varnothing$ by contradiction. So assume that there exists $x \in \bigcap_{i=1}^{\infty} B_i$. Then we would have $0 < x < \frac{1}{n}$ for all $n \in \N$. 
        But then by the Archimedian property we may fix $N \in \N$ such that for all $n > N$ we have $\frac{1}{n} < x $. Hence $x \notin B_n$. But since we assumed that $x \in \bigcap_{i = 1}^{\infty} B_i $
        we have a contradiction. Thus we see that $\bigcap_{i = 1}^{\infty} B_i = \varnothing$ must be true. Hence induction on $\N$ does not imply the infinite case. 


    \end{proof}




    \questionpart
    prove the infinte case of DeMorgan's laws. 

    \begin{proof}
        We want to prove that 

        \[\bigcap_{i = 1}^{\infty} A_i^c = \left(\bigcup_{i = 1}^{\infty} A_i \right)^c \]

        We proceed by showing that they are subsets of each other in the standard way. So assume that $x \in  \bigcap_{i = 1}^{\infty} A_i^c$
        Which implys that $x \notin A_i$ for all $i \in \N$. Hence $x \notin \bigcup_{i=1}^{\infty} A_i$. Then by defintion $x$ must be in the complement. 
        Now we assume that $x \in \left(\bigcup_{i=1}^{\infty} A_i \right)^c$ then we have that $x \notin \bigcup_{i=1}^{\infty} A_i $, Thus 
        $x \notin A_i$ for all $i \in \N$ which implys that $x \in A_i^c$ for all $i \in \N$. Then we get the desired result that $x \in \bigcap_{i=1}^{\infty} A_i^c$. 

    \end{proof}
\end{alphaparts}

\end{document}